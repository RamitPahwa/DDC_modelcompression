Title: Data Driven Compression of CNNs for On-Device Deployment

[Done] Abstract:
    3 questions -
    a) control trade-off between accuracy, memory footprint, speed (AMS trade-off) of compressed model
            doing this by hand designing architecture is very non-trivial
    b) improve AMS trade-off if only few classes need to be inferred
    c) scale to 100s of executions by compression policy transfer
    [Not done] experiments of on-device claims

[Done] Introduction:
* SOTA CNNs are large and deep. Need more memory footprint and slower in inference (price of high accuracy). Problematic for on-device deployments.
* Compression is one solution with many open questions
    * autonomous car example - don't have to label all classes accurately, even if trained on ImageNet
    * Compression takes time! personalized compression at scale is difficult!
* We propose an RL based compression algorithm (building on N2N [Ashok et. al.] paper). Contributions -
    a) design reward functions for accuracy, memory footprint, speed (speed is new addition)
    b) experimentally demo subset specific compression gives better AMS trade-off
    c) experimentally demo learned policy is transferable to other data subsets.
        Amortizes cost of personalized compression (to some extent).
        Distinguished from N2N [Ashok et. al.] paper's policy transfer.

[Done] Related Work:
* Depth reduction requires hand design of networks
a) Pruning -
    very little control over the compression process
b) Knowledge Distillation
    student model architecture needs to be hand designed
c) Architecture Search
    search within sub architectures of teacher network
    control of KPIs via reward functions
    Include distinction from N2N [Ashok et. al.] paper

Compression Algorithm:
[TO-DO] Rewrite policy network, Reward Functions, Compressed Model Search, Optimization.
a) policy Network :too similar to N2N
Add intro about the RL based approach and its contemporary E.A, B.O for searching the optimal architectures.
1. Rewrite reader is advised to read n2n, give less credit
2. Add citation of more paper using MDP process to compress, [Rephrase it better]
3. Remove shrinkage action, cite other actions papers like ADC [See Onenote for more]

1. Policy Network: To similar to the ashokâ€™s, Rewrite 
    a) Layer Removal policy 
    b) Bidirectional LSTM architecture 
    c) Layer representation 
    d) Remove layer shrinkage policy part 
2. Reward function 
    a) Add graphical representation of the reward function design
    b) Rewrite the details
    c) Compare against discontinuous reward function ( cut-off at threshold)  and our reward ?
    d) Clearly demonstrate the difference in reward construction from n2n and show our reward is better
3.Compressed Model Search : Same as N2N (maybe remove) also cite more paper using the same scheme.
4.Optimization : See other papers, this method is not original to n2n, so maybe we can use it as as.

Experiments:
* All datasets, algorithm implementations, experiment configurations and results are available at [point to anon link]
a) [Done] Datasets -
    * Fullsets - CIFAR-10, CIFAR-100
    * Subsets - created by combining few classes into a superclass
                we experiment with Animals and Vehicles superclasses (designate unique names for data subsets), and others
b) Methods Under Study -
    * mention paper and source code citation and hyperparameters for all methods here (if the hyperparameters don't change across experiments)
    * designate unique names for each method (also distinguish between different parameter settings only within same experiment)
    1. N2N
    2. Pruning
    3. KD
        * mention how student architecture is decided (if different in each experiment, mention in the respective experiment description)
    4. AutoKeras
    5. DDC (our method)
        Note: reward function thresholds are hyperparameters
        a. Reward only -
            * Include Reward function plots
        b. Reward + Loss -
            * mention what is different from "Reward only"
c) Details of Experiments -
    * We compare on accuracy, inference time, compression/memory and execution time
    * How inference time and memory footprints are measured
    [Done] Boldface the winning numbers in the tables
    [Todo] Tables(perhaps in text) are missing execution time
    * Describe EACH experiment, including
        * Which input model(s) on which data (sub)set
        * How train, validate, test split is done
            + how/what accuracy is measured
        * Express results in the form of table(s). Not as bar charts, since the numbers are close.
        * What is noteworthy in the results?
        1. Compressed Model Search -
            * All methods will be used for comparison
            [Todo] demo that different points on the AMS trade-off curve are achievable
        2. Data Subset based Compression -
            * For DDC method, contrast AMS trade-off numbers w.r.t. first experiment. Take care to calculate inference time and accuracy on the subset(s) only even in the first experiment output for this comparison to be meaningful.
                + Do this comparison for multiple (all?) methods only if it can be meaningfully done
        3. Compression with Policy Transfer -
            * For DDC method, contrast AMS trade-off + Execution time w.r.t. second experiment.
                + Could policy learned by N2N also be transferred in the same way? If yes, get the results of that experiment and run the same comparison w.r.t. the second experiment.
            * Include compression vs iterations & accuracy vs iterations plots w.r.t. compression from scratch

[Todo] Conclusions:
* Did not do -
    a. test with additive reward structure in RL
    b. on-device experiments/measurements