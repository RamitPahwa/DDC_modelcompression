%\section{Experiments}
%In this experiment, we have tried running N2N compression on the CIFAR 10 and CIFAR100 datasets by modifying the reward function.
%T(R\_a) and T(R\_c) represents the transformation of the reward function with respect to the accuracy and compression ratio of the compressed model.
%Loss function has been modified and KL Divergence loss has been used, which is calculated as weighted average of difference between teacher output and student output and student output and target.
%
%
%Experiments have been run on Vehicles and the Animals dataset by using three different loss function.
%Enhancement has been observed in accuracy, compression and inference time in some cases.
%
%Policy transfer has been experimented by learning the policy on one subset training dataset, We analysed the reusablility of policy learnt on animals to compress the model learnt on vehicles.
%Equivalent accuracy along with a reasonable compression ratios is obtained, but the time to converge in most cases has been reduced, cases in which convergence is not obtained , the results are consistent for the learnt policy as well 
%
%The Controller or policy learner is a Bi- directional LSTM which c
%
%On CIFAR100, policy transfer experiments have been run on Insect and Fruit datasets.
%2500 images have been taken for the policy training.
%Number of epochs have been increased for the Insect dataset and the changes in the accuracy, compression and the inference time has been observed in affirmative.

% TODO: Highlight the important results, and remove any redundant results
% \begin{table}
% \caption{Experimental Summary on Cifar-10 dataset}
% \centering
% \begin{tabular}{|c|p{1.3cm}|p{2cm}|p{1.8cm}|p{2cm}|}
%  \hline
%  Description & Accuracy & Compression & Inference Time & Per image inference Time\\
%  \hline
%  $1/\log\text{Time}$ & 0.7217 & 0.21987392 & 1.20781684 & pp \\
%  \hline
%  $R_a * R_c * 1/\log\text{Time}$ & 0.7561 & 0.366665  & 2.9950247 & pp \\
%  \hline
%  $T(R_a) * R_c$ & 0.721 & 0.29562 & 1.41918 & pp \\
%  \hline 
%  $T(R_a) * T(Time) * R_c$ & 0.7554 & 0.39 & 1.44365 & pp \\
%  \hline
%  $T(R_a) * T(Time)$ & 0.7172 & 0.18 & 1.19 & pp\\
%  \hline
%  \multicolumn{5}{|c|}{KL Divergence in Resnet  $[Animals]$ }\\
%  \hline
%  At high temperature: $T(R_a) * \text{T(Time)} * R_c$ & 0.732 & 0.32 & 0.834 & 0.14 \\
%  At low temperature: $T(R_a) * \text{T(Time)} * R_c$ & 0.69 & 0.5 & 1.0 & 0.166 \\
%  \hline
%  [Vehicles] [High Temperature] & 0.87 & 0.5 & 0.648 & 0.162\\
%  \hline
%  Policy Transfer: Animals to Vehicles & 0.78 & 0.2 & 0.524 & 0.125+\\
%  \hline
%  Policy Transfer: Vehicles to Animals & 0.701 & 0.32 & 0.86 & 0.14\\
% \hline 
%  Vehicles + New Loss & 0.876 & 0.54 & 0.5151 & 0.5151/3 = 0.1717 \\
% \hline 
%  Vehicles + New Loss & 0.86 & 0.45 & 0.47 & 0.47/3 = 0.1717 \\
% \hline 
%  Animals + New Loss & 0.738 & 0.52 & 0.48 & 0.48/3 = 0.16 \\
% \hline 
%  Vehicles + Swapped Controller & 0.81 & 0.18 & 0.38 & 0.38/3 = 0.13\\
% \hline 
%  Vehicles + Swapped Controller & 0.828 & 0.43 & 0.39 & 0.39/3 = 0.13\\
% \hline 
%  Animals + Swapped Controller & 0.72 & 0.50 & 0.44 & 0.44/3 = 0.146\\
%  \hline 
%  Animals + Swapped Controller & 0.65-0.69 & 0.184 & 0.40 & 0.40/3 = 0.133\\
%  \hline
% \end{tabular}
% \end{table}

%\begin{table}
%\caption{Experimental Summary on Cifar 100 dataset}
%\centering
%\begin{tabular}{|r|p{1.3cm}|p{2cm}|p{1.8cm}|p{2cm}|}
%\hline
%Description\hspace{1 cm} & Accuracy & Compression & Inference Time & Per image inference Time  \\
%\hline 
%Insects [2500 Images] & 0.54 & 0.52 & 0.078 & pp   \\
%\hline 
%Insects [Transferred][Fruits] & 0.542 & 0.189 & 0.06139 & pp   \\
%\hline 
%Insects + high epoch & 0.6687 & 0.189/0.47 & 0.0666 & pp   \\
%\hline 
%Fruits & 0.68 & 0.189 & 0.062 & pp \\
%\hline 
%Fruits [Transferred][Insects] & 0.672 & 0.189 & 0.062 & pp \\
%\hline 
%\end{tabular}
%\end{table}

%\begin{table}
%\caption{Experimental Summary on Cifar 100 dataset}
%\centering
%\begin{tabular}{|r|p{1.3cm}|p{2cm}|p{1.8cm}|p{2cm}|}
%\hline
%Description\hspace{1 cm} & Accuracy & Compression & Inference Time & Per image inference Time  \\
%
%
%\hline 
%Insects [2500 Images] & 0.54 & 0.52 & 0.078 & pp   \\
%\hline 
%Insects [Transferred][Fruits] & 0.542 & 0.189 & 0.06139 & pp   \\
%\hline 
%Insects + high epoch & 0.6687 & 0.189/0.47 & 0.0666 & pp   \\
%\hline 
%Fruits & 0.68 & 0.189 & 0.062 & pp \\
%\hline 
%Fruits [Transferred][Insects] & 0.672 & 0.189 & 0.062 & pp \\
%\hline 
%\end{tabular}
%\end{table}
%
%
%
%\begin{table}
%\caption{Experimental Summary on Cifar-10 dataset}
%\centering
%\begin{tabular}{|c|p{1.3cm}|p{2cm}|p{1.8cm}|p{2cm}|}
%\hline
%Description & Accuracy & Compression & Inference Time & Per image inference Time   \\
%\hline
%$1/\log{(\text{Time})}$ & 0.7217 & 0.21987392 & 1.20781684 & pp    \\
%\hline
%$R_a * R_c * 1/\log{(\text{Time})}$ & 0.7561 & 0.366665  & 2.9950247 & pp  \\
%\hline
%$T(R_a) * R_c$ & 0.721 & 0.29562 & 1.41918 & pp    \\
%\hline 
%$T(R_a) * T(Time) * R_c$ & 0.7554 & 0.39 & 1.44365 & pp    \\
%\hline
%$T(R_a) * T(Time)$ & 0.7172 & 0.18 & 1.19 & pp \\
%\hline
%\multicolumn{5}{|c|}{KL Divergence in Resnet  $[Animals]$ }    \\
%\hline
%At high temperature: $T(R_a) \text{T(Time)} * R_c$ & 0.732 & 0.32 & 0.834 & 0.14   \\
%
%At low temperature: $T(R_a) * \text{T(Time)} * R_c$ & 0.69 & 0.5 & 1.0 & 0.166 \\
%\hline
%[Vehicles] [High Temperature] & 0.87 & 0.5 & 0.648 & 0.162 \\
%\hline
%Policy Transfer: Animals to Vehicles & 0.78 & 0.2 & 0.524 & 0.125+ \\
%\hline
%Policy Transfer: Vehicles to Animals & 0.701 & 0.32 & 0.86 & 0.14  \\
%\hline 
%Vehicles + New Loss & 0.876 & 0.54 & 0.5151 & 0.5151/3 = 0.1717    \\
%\hline 
%Vehicles + New Loss & 0.86 & 0.45 & 0.47 & 0.47/3 = 0.1717 \\
%\hline 
%Animals + New Loss & 0.738 & 0.52 & 0.48 & 0.48/3 = 0.16   \\
%\hline 
%Vehicles + Swapped Controller & 0.81 & 0.18 & 0.38 & 0.38/3 = 0.13 \\
%\hline 
%Vehicles + Swapped Controller & 0.828 & 0.43 & 0.39 & 0.39/3 = 0.13    \\
%\hline 
%Animals + Swapped Controller & 0.72 & 0.50 & 0.44 & 0.44/3 = 0.146 \\
%\hline 
%Animals + Swapped Controller & 0.65-0.69 & 0.184 & 0.40 & 0.40/3 = 0.133   \\
%\hline
%\end{tabular}
%\end{table}


%\subsection{Discussion}
%Most of the layer removal happens from the first few layer , implying the presence of redundant information for the various subset of dataset, and is dropped by the policy.
%The process removes redundant layer on the basis of data input, this reinforcement scenario is similar to hyperparameter search [cite NAS].
%Policy learnt on dataset can be used to speed up the compression of the model for other dataset as well, through experiments it is evident that there is scope of learning a global policy which helps to improve compression with little drop in accuracy.
%Some classes perform better, which might be due to the fact that the intra-class variance of the selected dataset may be high and with limited amount of data, the trained classifier performs in acceptable ranges.
%Each Architecture is trained using Knowledge distillation.
%Loss function is the KL divergence loss which is calculated as weighted average of difference between teacher output and student output , and student output and target.
%
%
%\section{Experiments}
%Our goal is to do data specific model compression of deep learning models.
%We believe that data specific compression is superior to ad-hoc compression because it allows the model architecture to be tailored for the specific distribution of data from which the dataset is drawn from.
%Intuition suggests that since the model only needs to perform well on samples drawn from the dataset distribution, we believe that we can compress more while still maintaining high accuracy figures.
%To verify this claim, we used N2N to compress ResNet-18 on different sets of data.
%To model our situation, we divided the CIFAR10 and CIFAR100 datasets to form smaller subset datasets.
%These smaller subset datasets contain classes that are 'similar'; for example, we split CIFAR10 to two datasets, one which contained animals (bird, cat, dog, deer, etc) and another which contained vehicles (airplane, truck, ship, automobile).

%\subsection{Reward functions}
%The original N2N paper uses a combination of accuracy and compression to create a reward function.
%However, we noticed that models with fewer parameters need not necessarily have the fastest inference time.
%To remedy this, we add an additional time factor in our reward function.
%The final functional form of our reward are as follows.
%
%The following reward curves are used to represent the effect of :
%
%\begin{itemize}
%    \item Reward v/s Accuracy \[ 
%    \begin{cases} 
%      \ln{\frac{1+e^{a-a_{th}}}{2}} & a_{th}\leq a\leq 1   \\
%      \frac{a}{2} & 0 \leq a \leq a_{th} 
%   \end{cases}
%\] 
%    \item Reward v/s Time $$ R(t) =  \frac{\mathrm{1} }{\mathrm{1} + e^{(t-t_{th})*10} }  $$ 
%    \item Reward v/s Compression $$R(c) = c * (2 -c)$$
%    \item $a$ is  actually ratio of $\frac{Accuracy}{BaselineAccuracy}$ , $a_{th}$ is set to 0.65 and $t_{th}$ i set to 3 sec, baseline accuracy is calculated and compression is defined as $\frac{number of parameters of student}{\#number of of parameters of teacher}$
%\item New Reward v/s Accuracy \[ 
%    \begin{cases} 
%      \ln{\frac{1+e^{a-a_{th}}}{2}} & 0\leq a\leq a_{th1}  \\
%      \frac{a}{2} & a_{th1} \leq a \leq a_{th2} 
%   \end{cases}
%\] 
%\end{itemize}
