\documentclass[../main]{subfiles}
\begin{document}
\section{Experimental Results}
    \label{sec:expts}
    Searching a architecture \cite{zoph2017learning,tan2018mnasnet} from scratch has its limitation as it takes unrealistic time to search an optimal architecture for large datasets. 
    Our approach is similar to \cite{ashok2017n2n,molchanov2016pruning} as we start with a high performance teacher model and then compress it to realistic sized student models.
    We test our approach on publicly available datasets CIFAR 10, CIFAR100 as well as test our approach on subsets on data to provide evidence towards \textit{personalisation} Detailed description on how we construct our subsets is provided in \textbf{hyperlink}Section 4.1.
    We compare the proposed reinforcement method with heuristic based pruning \cite{molchanov2016pruning,lecun1990optimal}, Knowledge distillation \cite{hinton2015distilling} and Bayesian Optimization \cite{jin2018efficient} in \textbf{hyperlink} Section 4.3.
    We also empirically demonstrate the effect of policy transfer amongst dataset in \textbf{hyperlink} Section 4.4.
    In the spirit of reproducibility, we have made all datasets, implementations for experiments, and results available\footnote{\url{http://bit.ly/2BTduY5}}. 
    
    \subsection{Datasets}
        \label{sec:datasets}
        We use the CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} datasets in our experiments.
        We will also work with specific subsets of these datasets for some of the experiments which are described below. The subsets used in the experiments are:
        \begin{enumerate}
            \item   \texttt{Animals}: Subset of CIFAR-10 formed by the class labels \texttt{bird}, \texttt{cat}, \texttt{deer}, \texttt{dog}, \texttt{frog} and \texttt{horse}.
            \item   \texttt{Vehicles\textsubscript{10}}: Subset of CIFAR-10 formed by the class labels \texttt{airplane}, \texttt{automobile}, \texttt{ship} and \texttt{truck}.
            \item   The superclasses \texttt{Insects}, \texttt{Fruits}, \texttt{Trees}, \texttt{Vehicles-1}, \texttt{Vehicles-2}, \texttt{People}, \texttt{Reptiles}
        \end{enumerate}
        We use the superclasses given the CIFAR-100 dataset as subsets.
        
            
        The description of the data set is tabulated in \tablename~\ref{tab:datasets}.
    
        \subfile{../tables/datasets}
    \subsection{Implementation details}
    Our experiments provide evidence towards efficacy of imposing thresholds on reward to improve the architecture search.
    One important point to note here is that we do not use any proxies for inference time (like FLOPS) but instead we use actual inference time on system.
    For the memory footprint, we use number of parameters present in the model.
    Furthermore for consistency, we use the same teacher models across methods to provide evidence towards efficacy of our system.
    We use stochastic gradient descent with $momentum = 0.9 $ and $learning rate = 0.001$ for all our experiments.
    Formally, given a model $m$, we define the reward functions for the in terms of the accuracy $\bb{A}$, inference time $\bb{T}$ and compression $\bb{C}$.

    The results obtained from the changes are tabulated in \tablename~\ref{tab:ResNet18-CIFAR10}.

    \subfile{../tables/res18-c10}

    \subfile{../tables/res18-c100}

    \subfile{../tables/vgg11-c10}

    \subsection{Methods Under Study}
        \label{sec:baselines}
        
        \begin{enumerate}
            \item   \texttt{N2N}: The system systematically finds a compressed optimal architecture by searching within the teacher's architecture~\cite{ashok2017n2n}.
            The number of reinforcement learning iterations have been fixed 100, where in each iteration we train 5 new student architecture using Knowledge distillation.
            \item   \texttt{Prun}: Heuristic based removal of filters~\cite{molchanov2016pruning}.
            We remove 512 filter on each iteration of pruning followed by 10 epochs of fine-tuning to recover the network.
            \item   \texttt{KD}: \cite{hinton2015distilling} This works towards transferring the \textit{dark knowledge} to user defined student architecture, which is 5 layer deep CNN architecture \textbf{increase depth of the CNN} inspired by the VGG architecture \cite{simonyan2014very} in our experiments.
            We then employ same training strategy than we use in our proposed method defined in \textbf{hyperlink} Section 3.4. 
            \item   \texttt{AK}: Auto-Keras \cite{jin2018efficient} is popular tool for architecture search which uses Bayesian optimization and searches the model from scratch.
            We run experiments on our subset of data as well as CIFAR10,CIFAR100 and find that architectures searched using our proposed method outperforms architectures searched from scratch and takes less time for the search to reach convergence.
            
            \item   \texttt{DDC}: 
        \end{enumerate}
        
        Our system starts with complex Teacher network and systematically reduced it to compressed architecture using reinforcement learning, this process is inspired by \cite{ashok2017n2n}.
        This process is compared to the popular ranking based pruning method \cite{molchanov2016pruning}.
        We utilize the same teacher models as starting point and systematically prune the network down, alternating between pruning, which prunes the network on the basis of heuristics and then followed by fine-tuning it for 10 epochs.
        The models generated have sub-optimal architectures and exploration of the optimal architectural space.
        The heuristic based pruning is compared to our method across architectures on subsets of data to provide evidence towards personalization of complex models to smaller efficient CNN classifiers.
        The results are presented in \tablename~\ref{tab:ResNet18-CIFAR10_subset+transfer}.
    
        Another popular approach, when compressing down from a \textit{over-learnt} model is knowledge distillation \cite{hinton2015distilling}.
        Our approach utilizes the \textit{dark knowledge} learnt by large teacher model and uses it as signal to better train our student model.
        One obvious pit fall to this approach is that you need to fix the student architecture you distill the knowledge into, in contrast to this the proposed method uses reinforcement learning to first find candidate student architecture and then using knowledge distillation to train the student networks.
        We then compare the performance our method with popular library AutoKeras~\cite{jin2018efficient} which searches for architectures from scratch based on Bayesian Optimization.
        We found that the library takes almost as \textbf{time in comparison to ours} to find a architecture which is sub-optimal in performance.
        Therefore, restricting the architecture search amongst only popular teacher models can significantly reduce time to search efficient architectures for given dataset.
    
    
    \subsection{Policy Generalization}
        \label{sec:pg}
        We also wanted to test the generalizability of the policy learned by the controller on different datasets and across different models.
        In the real world, datasets are very often very large, leading to prohibitively long times to perform the neural architecture search.
        Something we noticed was that in the real world, data is not distributed evenly across people.
        Different sets of people have diffenent types of data.
        So to test whether the controller truly learnt to compress, we must verify that after training on one dataset, we get similar accuracy on another dataset.
        This would mean that the policy learnt on one dataset could be used to bootstrap the learning of the policy on another related dataset.
        To test this, we devised an experiment where we split the training dataset into equal sized subsets.
        We do not spilt into subsets randomly, but we choose the subsets such that the subsets are drawn from similar distributions.
        For example, we divide the cifar 10 dataset into two subsets, one for Animals and one for Vehicles.
        We then take the controller learnt on one datset on the other dataset. The results for this experiment are tabulated in \tablename~\ref{tab:ResNet18-CIFAR100}.

        We train the controller for 100 iterations, and in each iteration we train for 20 epochs.
        We do not train till convergence for the sake of time. In all cases, we used the 4th loss function from the top of \tablename~\ref{tab:ResNet18-CIFAR10}. % TODO: Mention exact numbers for droupout, momentum, etc.
        In each iteration, we train the model using Knowledge Distillaiton, where the we use the original parent model from which we compress as the teacher model.
        The KD loss is a weighted average of the difference between the teacher output and student output, and the student and target output.

        \subfile{../tables/res18-c10_exp-2+3}
        \subfile{../tables/vgg11-c10_exp-2+3}
        
        From the results, we see that the model using the transferred policy performs worse than when the model is learned from scratch.
        However, the loss in accuracy is not very great (only about 5\%) and could very well be attributed to the shorter time for which we train our controller.
        The models outputted are all the top performing models we get after running our architecture search algorithm.
        We also notice that the difference in accuracy between original and swapped controller is much greater for some classes than it is for others.
        This might be because the intra-class variance of the selected dataset on which we train is high, and with limited amount of data the classifier performs in acceptable ranges. 

    \bibsubfile{named}{bibs/sub}
\end{document}
